{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPFqW9sRvmWm","executionInfo":{"status":"ok","timestamp":1717617826516,"user_tz":-120,"elapsed":34159,"user":{"displayName":"ROCIO GILABERT PINA","userId":"05559686019425400606"}},"outputId":"95521303-3cb6-4599-829c-05a76b6f9758"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Conv2D, UpSampling2D, concatenate, Input\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Flatten, Dense, Input, GlobalAveragePooling2D"],"metadata":{"id":"cYjqs81Bv62W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n","from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","from PIL import Image\n","import tifffile as tiff"],"metadata":{"id":"kDSiQ6OMxKhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data directories\n","data_dir = '/content/drive/Shareddrives/DeepLearning_2024/FinalProject/Data'\n","\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')\n","\n","# Load and preprocess the data\n","def load_data_from_directory(base_dir, img_size=(224, 224)):\n","    categories = ['polyps', 'non_polyps']\n","    images = []\n","    labels = []\n","\n","    for category in categories:\n","        path = os.path.join(base_dir, category)\n","        class_num = categories.index(category)\n","\n","        for img_name in os.listdir(path):\n","            img_path = os.path.join(path, img_name)\n","            img = load_img(img_path, target_size=img_size)\n","            img = img_to_array(img) / 255.0\n","            images.append(img)\n","            labels.append(class_num)\n","\n","    images = np.array(images)\n","    labels = np.array(labels)\n","\n","    # Standardize images\n","    mean = np.mean(images)\n","    std = np.std(images)\n","    images = (images - mean) / std\n","\n","    return images, labels\n","\n","# Load training and test data\n","train_images, train_labels = load_data_from_directory(train_dir)\n","test_images, test_labels = load_data_from_directory(test_dir)\n","\n","# Split training data into training and validation sets\n","train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)"],"metadata":{"id":"lz2rSXCVwGZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG16 model for transfer learning\n","vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Adding custom layers for classification\n","inputs = Input(shape=(224, 224, 3))\n","base_model = vgg16(inputs)\n","x = GlobalAveragePooling2D()(base_model)\n","x = Dense(256, activation='relu')(x)\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","vgg16_model = Model(inputs, outputs)\n","\n","# Freeze all layers except the last two dense layers\n","for layer in vgg16_model.layers[:-4]:\n","    layer.trainable = False\n","\n","# Compile the model\n","vgg16_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    fill_mode='nearest')\n","\n","# Train the model with data augmentation\n","vgg16_history = vgg16_model.fit(train_datagen.flow(train_images, train_labels, batch_size=8),\n","                                epochs=50,\n","                                validation_data=(val_images, val_labels))\n","\n","# Evaluate the model\n","loss, accuracy = vgg16_model.evaluate(test_images, test_labels)\n","print(f'Loss: {loss}, Accuracy: {accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"kLndz_sFwMSG","executionInfo":{"status":"error","timestamp":1717619486241,"user_tz":-120,"elapsed":1375952,"user":{"displayName":"ROCIO GILABERT PINA","userId":"05559686019425400606"}},"outputId":"1301d891-7b63-4af8-dc51-a4d91cf8870d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n","Epoch 1/50\n","88/91 [============================>.] - ETA: 46s - loss: 0.5901 - accuracy: 0.6662 "]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-88f0508e7933>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train the model with data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m vgg16_history = vgg16_model.fit(train_datagen.flow(train_images, train_labels, batch_size=8),\n\u001b[0m\u001b[1;32m     33\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                 validation_data=(val_images, val_labels))\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["plot_history(vgg16_history)"],"metadata":{"id":"86zatX60w24Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Function to resize images\n","def resize_images(images, size=(160, 160)):\n","    resized_images = np.zeros((images.shape[0], size[0], size[1], 3))\n","    for i in range(images.shape[0]):\n","        resized_img = array_to_img(images[i]).resize(size)\n","        resized_images[i] = img_to_array(resized_img)\n","    return resized_images\n","\n","# Resize train, validation, and test images\n","train_images_resized = resize_images(train_images, size=(160, 160))\n","val_images_resized = resize_images(val_images, size=(160, 160))\n","test_images_resized = resize_images(test_images, size=(160, 160))\n","\n","# InceptionV3 model for transfer learning\n","inceptionv3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(160, 160, 3))\n","\n","# Adding custom layers for classification\n","inputs = Input(shape=(160, 160, 3))\n","base_model = inceptionv3(inputs)\n","x = GlobalAveragePooling2D()(base_model)\n","x = Dense(256, activation='relu')(x)\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","inceptionv3_model = Model(inputs, outputs)\n","\n","# Freeze all layers except the last two dense layers\n","for layer in inceptionv3_model.layers[:-4]:\n","    layer.trainable = False\n","\n","# Compile the model\n","inceptionv3_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    fill_mode='nearest')\n","\n","# Train the model with data augmentation\n","inceptionv3_history = inceptionv3_model.fit(\n","    train_datagen.flow(train_images_resized, train_labels, batch_size=8),\n","    epochs=50,\n","    validation_data=(val_images_resized, val_labels))\n","\n","# Evaluate the model\n","loss, accuracy = inceptionv3_model.evaluate(test_images_resized, test_labels)\n","print(f'Loss: {loss}, Accuracy: {accuracy}')\n"],"metadata":{"id":"37NQ1OXBwkwh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(inceptionv3_history)\n"],"metadata":{"id":"xw9fNK6Cwx-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ResNet50 model for transfer learning\n","resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Adding custom layers for classification\n","inputs = Input(shape=(224, 224, 3))\n","base_model = resnet50(inputs)\n","x = GlobalAveragePooling2D()(base_model)\n","x = Dense(256, activation='relu')(x)\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","resnet50_model = Model(inputs, outputs)\n","\n","# Freeze all layers except the last two dense layers\n","for layer in resnet50_model.layers[:-4]:\n","    layer.trainable = False\n","\n","# Compile the model\n","resnet50_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    fill_mode='nearest')\n","\n","# Train the model with data augmentation\n","resnet50_history = resnet50_model.fit(train_datagen.flow(train_images, train_labels, batch_size=8),\n","                                      epochs=50,\n","                                      validation_data=(val_images, val_labels))\n","\n","# Evaluate the model\n","loss, accuracy = resnet50_model.evaluate(test_images, test_labels)\n","print(f'Loss: {loss}, Accuracy: {accuracy}')\n"],"metadata":{"id":"f25lycMBwgf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(resnet50_history)\n"],"metadata":{"id":"4VW3yn_GwXyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pre-trained VGG16 model\n","#model = vgg16_model\n","#model = resnet50_model\n","model = inceptionv3_model\n","\n","image_path = data_dir + '/144.tif'\n","\n","# Load the image using tifffile\n","try:\n","    img = tiff.imread(image_path)\n","    print(f\"Image loaded successfully. Shape: {img.shape}\")\n","except Exception as e:\n","    print(f\"Failed to load image: {e}\")\n","    img = None\n","\n","\n","# If the image is grayscale, convert it to BGR\n","if len(img.shape) == 2 or img.shape[2] == 1:\n","    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n","    print(\"Converted grayscale image to BGR.\")\n","\n","# Resize the image to ensure it fits the sliding window process\n","if img.shape[0] < 224 or img.shape[1] < 224:\n","    img = cv2.resize(img, (224, 224))\n","    print(f\"Resized image to: {img.shape}\")\n","\n","# Define sliding window parameters\n","WINDOW_SIZE = 150\n","step = 10\n","\n","# Initialize variables to store the best prediction\n","max_pred = 0.0\n","max_box = []\n","\n","print('--> Searching for a colonoscopy polyp ...')\n","\n","# Loop through the image using sliding window\n","for top in range(0, img.shape[0] - WINDOW_SIZE + 1, step):\n","    for left in range(0, img.shape[1] - WINDOW_SIZE + 1, step):\n","        # Compute the coordinates of the bounding box\n","        box = (top, left, top + WINDOW_SIZE, left + WINDOW_SIZE)\n","\n","        # Crop the original image\n","        cropped_img = img[box[0]:box[2], box[1]:box[3], :]\n","        # Resize the cropped image to match the input size expected by the model\n","        cropped_img_resized = cv2.resize(cropped_img, (160, 160))\n","\n","        # Normalize the resized cropped image\n","        # cropped_img_resized = cropped_img * 1.0 / 255\n","        cropped_img_resized = cropped_img_resized * 1.0 / 255\n","        # Reshape for prediction\n","        cropped_img_resized = cropped_img_resized.reshape((1, cropped_img_resized.shape[0], cropped_img_resized.shape[1], cropped_img_resized.shape[2]))\n","\n","        # Make prediction for the resized cropped image\n","        preds = model.predict(cropped_img_resized)\n","\n","        if preds[0][0] > max_pred:\n","            max_pred = preds[0][0]\n","            max_box = box\n","\n","print('Done!')\n","print('Best prediction:', max_box, max_pred)\n","\n","# Visualize the best prediction box on the original image\n","plt.figure()\n","plt.imshow(img)\n","plt.text(1, -5, 'Best probability: ' + str(max_pred), fontsize=10)\n","currentAxis = plt.gca()\n","if max_box:  # Check if max_box is not empty\n","    currentAxis.add_patch(Rectangle((max_box[1], max_box[0]), WINDOW_SIZE, WINDOW_SIZE, linewidth=1, edgecolor='r', facecolor='none'))\n","plt.show()\n"],"metadata":{"id":"chFqcds-ws3y"},"execution_count":null,"outputs":[]}]}